{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3ZWP0CcOjYO2LHXGZq43G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rodrigoradzinski/RodrigoRadzinki-Projeto-de-Circuitos-Digitais-IA-Predict_Match_World_Cups_2022-/blob/main/TRABALHO%20DE%20SEGMENTA%C3%87%C3%83O%20DE%20VIDEO_2\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import os\n",
        "import tarfile                   \n",
        "import tempfile                  \n",
        "from six.moves import urllib     \n",
        "from io import BytesIO           \n",
        "\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import IPython\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import ipyplot\n",
        "import tensorflow as tf  # Adicionado\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Image, clear_output\n",
        "from io import BytesIO\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "w07uEBFdRMB3"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'people6.mp4' 'https://github.com/Rodrigoradzinski/Segmenta-o-de-videos/raw/main/people6.mp4'"
      ],
      "metadata": {
        "id": "Qtj0w8LbQNW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_colormap():\n",
        "    \"\"\"\n",
        "    Retorna:\n",
        "        Array numpy com o mapa de cores para visualizar resultados da segmentação. \n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [120, 155,  42],\n",
        "        [152, 251, 152],\n",
        "        [ 93, 165, 227],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [ 34,  34, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap"
      ],
      "metadata": {
        "id": "OGzymPikoKE0"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_NAMES = np.asarray([\n",
        "    'rua', 'faixa de pedestres', 'prédios', 'muro', 'cerca', 'poste', 'luz de trânsito',\n",
        "    'sinal de trânsito', 'mato/ vegetação', 'terreno', 'céu', 'pessoa', 'motorista', 'carro', 'caminhão',\n",
        "    'ônibus', 'tren', 'moto', 'bicicleta', 'vazio'])\n",
        "\n",
        "len(LABEL_NAMES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b_ztbOMoPe7",
        "outputId": "2e5dc1ea-78e8-460f-8dc4-c4d09d64fb54"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def executa_segmentacao(caminho):\n",
        "  img_original = Image.open(caminho)\n",
        "  seg_map = model.run(img_original)\n",
        "  img_processada = vis_segmentacao(img_original, seg_map)\n",
        "  return img_processada"
      ],
      "metadata": {
        "id": "X_bT8MnLowOl"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1) # IDs das classes  \n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)                     # cores associadas aos IDs\n",
        "\n",
        "print(FULL_LABEL_MAP, FULL_COLOR_MAP)"
      ],
      "metadata": {
        "id": "m1LL5PU10XZu",
        "outputId": "d075501d-cd92-45cd-d4a3-5a686558ced2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0]\n",
            " [ 1]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 8]\n",
            " [ 9]\n",
            " [10]\n",
            " [11]\n",
            " [12]\n",
            " [13]\n",
            " [14]\n",
            " [15]\n",
            " [16]\n",
            " [17]\n",
            " [18]\n",
            " [19]] [[[128  64 128]]\n",
            "\n",
            " [[244  35 232]]\n",
            "\n",
            " [[ 70  70  70]]\n",
            "\n",
            " [[102 102 156]]\n",
            "\n",
            " [[190 153 153]]\n",
            "\n",
            " [[153 153 153]]\n",
            "\n",
            " [[250 170  30]]\n",
            "\n",
            " [[220 220   0]]\n",
            "\n",
            " [[120 155  42]]\n",
            "\n",
            " [[152 251 152]]\n",
            "\n",
            " [[ 93 165 227]]\n",
            "\n",
            " [[220  20  60]]\n",
            "\n",
            " [[255   0   0]]\n",
            "\n",
            " [[ 34  34 142]]\n",
            "\n",
            " [[  0   0  70]]\n",
            "\n",
            " [[  0  60 100]]\n",
            "\n",
            " [[  0  80 100]]\n",
            "\n",
            " [[  0   0 230]]\n",
            "\n",
            " [[119  11  32]]\n",
            "\n",
            " [[  0   0   0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloDeepLab(object):\n",
        "    \"\"\"Classe para carregar o modelo deeplab e fazer a inferência.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\" Cria e carrega o model deeplab pré-treinado. \"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extrai os frozen graph do arquivo tar.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Nao foi possivel encontrar o inference graph no arquivo tar.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Roda a inferência em uma imagem. \n",
        "\n",
        "        Parâmetros: \n",
        "            image: Objeto PIL.Image que contém a imagem de input. \n",
        "            INPUT_TENSOR_NAME: O nome do tensor de entrada. padrão=ImageTensor\n",
        "            OUTPUT_TENSOR_NAME: O name do tensor de saída. padrão=SemanticPredictions\n",
        "\n",
        "        Returns:\n",
        "            resized_image: imagem de entrada RGB redimensionada  \n",
        "            seg_map: Mapa de segmentação do `resized_image` \n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # tamanho das imagens do dataset Cityscapes\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        \n",
        "        seg_map = batch_seg_map[0]  # espera o batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # adiciona uma dimensão extra, necessária pro cv2.resize\n",
        "\n",
        "        seg_map = cv2.resize(seg_map, (width,height), interpolation=cv2.INTER_NEAREST)\n",
        "        return seg_map\n",
        "\n",
        "TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.io.gfile.makedirs(model_dir)\n",
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine' : 'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine'       : 'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "download_path = os.path.join(model_dir, TARBALL_NAME)\n",
        "download_path\n",
        "urllib.request.urlretrieve(DOWNLOAD_URL_PREFIX + MODEL_URLS[MODEL_NAME], download_path)\n",
        "model = ModeloDeepLab(download_path)"
      ],
      "metadata": {
        "id": "Ev6v6tKFQtkZ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "l6C8FG-7QIxn",
        "outputId": "ca2fc88d-921c-499c-924b-ba9624a313e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-d46f519a12cf>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Executar o modelo de segmentação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mseg_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubtracao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-4efab346dfa4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, image, INPUT_TENSOR_NAME, OUTPUT_TENSOR_NAME)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         batch_seg_map = self.sess.run(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mOUTPUT_TENSOR_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carregar e processar o vídeo\n",
        "kernel_size = 5\n",
        "cap = cv2.VideoCapture('/content/people14.mp4')\n",
        "subtracao = cv2.createBackgroundSubtractorMOG2()\n",
        "\n",
        "# Carregar o modelo de segmentação\n",
        "# Use a variável 'model' que já foi criada com a classe ModeloDeepLab\n",
        "modelo = model\n",
        "arrayw = []\n",
        "arrayh = []\n",
        "frames = []\n",
        "frame_count = 0  # contador de frames\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    if frame_count < 20:\n",
        "        frame_count += 1\n",
        "        continue\n",
        "    elif frame_count >= 30:\n",
        "        break\n",
        "    \n",
        "    # Redimensionar a frame para ser 50% menor\n",
        "    frame = cv2.resize(frame, None, fx=0.2, fy=0.2)\n",
        "\n",
        "    # Converter o frame para uma imagem PIL para usar com o modelo de segmentação\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Executar o modelo de segmentação\n",
        "    seg_map = modelo.run(pil_image)\n",
        "\n",
        "    fg_mask = subtracao.apply(frame)\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    opening = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
        "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
        "    dilat = cv2.dilate(closing, np.ones((10, 10)))\n",
        "    dilatacao = cv2.dilate(dilat, np.ones((3, 3), np.uint8))\n",
        "    contours, hierarchy = cv2.findContours(dilatacao, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # Converter o frame para tons de cinza\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Limiarização ToZero\n",
        "    _, thresholded_frame = cv2.threshold(dilatacao, 127, 255, cv2.THRESH_TOZERO)\n",
        "\n",
        "# Limiarização ToZero invertido\n",
        "    _, inverted_frame = cv2.threshold(gray_frame, 127, 255, cv2.THRESH_TOZERO_INV)\n",
        "\n",
        "\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if area > 1000:\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            if w >= 10 and h >= 140:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                arrayw.append(w)\n",
        "                arrayh.append(h)\n",
        "\n",
        "\n",
        "    frames.append(frame)\n",
        "    frame_count += 1  # aumenta o contador de frames\n",
        "\n",
        "    # Função para exibir a sobreposição da segmentação\n",
        "    def vis_segmentacao(image, seg_map):\n",
        "        seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "\n",
        "        plt.figure(figsize=(30, 20))\n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        ax1.imshow(image)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title('Imagem original')\n",
        "\n",
        "        ax1 = plt.subplot(3, 3, 2)\n",
        "        ax1.imshow(gray_frame)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title('Imagem gray_frame')\n",
        "\n",
        "        ax1 = plt.subplot(3, 3, 3)\n",
        "        ax1.imshow(thresholded_frame)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title('Imagem thresholded_frame')\n",
        "\n",
        "\n",
        "        ax1 = plt.subplot(3, 3, 3)\n",
        "        ax1.imshow(opening)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title('Imagem opening')\n",
        "\n",
        "        \n",
        "        ax1 = plt.subplot(3, 3, 5)\n",
        "        ax1.imshow(dilatacao)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title('Imagem dilatacao')\n",
        "\n",
        "\n",
        "        ax2 = plt.subplot(3, 3, 6)\n",
        "        ax2.imshow(seg_image)\n",
        "        ax2.axis('off')\n",
        "        ax2.set_title('Mapa de segmentação')\n",
        "\n",
        "        ax3 = plt.subplot(3, 3, 7)\n",
        "        ax3.imshow(image)\n",
        "        ax3.imshow(seg_image, alpha=0.7)\n",
        "        ax3.axis('off')\n",
        "        ax3.set_title('Sobreposição da segmentação')\n",
        "\n",
        "        plt.subplot(3, 3, 8)\n",
        "        plt.imshow(seg_map)\n",
        "         #plt.title('Imagem Segmentada')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        unique_labels = np.unique(seg_map)\n",
        "        ax4 = plt.subplot(3, 3, 9)\n",
        "        ax4.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "        ax4.yaxis.tick_right()\n",
        "        ax4.yaxis.set_label_position('right')\n",
        "        ax4.set_yticks(range(len(unique_labels)))\n",
        "        ax4.set_yticklabels(LABEL_NAMES[unique_labels])\n",
        "        ax4.set_xticks([])\n",
        "        ax4.tick_params(width=0.0)\n",
        "        ax4.grid(False)\n",
        "        plt.show()\n",
        "\n",
        "    # Chamar a função para exibir a sobreposição da segmentação\n",
        "    vis_segmentacao(frame, seg_map)\n",
        "\n",
        "    clear_output(wait=True)  # limpa a saída para o próximo frame\n",
        "mean_w = np.mean(arrayw)\n",
        "mean_h = np.mean(arrayh)\n",
        "print(\"Média da largura (w):\", mean_w)\n",
        "print(\"Média da altura (h):\", mean_h)\n",
        "cap.release()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # codec do vídeo\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (width, height))  # nome do arquivo de saída, codec, fps e tamanho do frame\n",
        "\n",
        "for frame in frames:  # para cada frame na lista de frames\n",
        "    out.write(frame)  # escreva o frame no arquivo de vídeo\n",
        "\n",
        "out.release()  # finalize a gravação e feche o arquivo"
      ],
      "metadata": {
        "id": "h82dTXc8a1Wb"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}